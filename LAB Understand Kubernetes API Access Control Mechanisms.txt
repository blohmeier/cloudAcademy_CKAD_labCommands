Introduction
You will use a multi-node Kubernetes cluster to explore authentication, authorization and admission control in this lab. This lab provides you with a Kubernetes cluster initialized with kubeadm and running on Ubuntu. The cluster is similar to the clusters used in official CNCF Kubernetes certification exams. It has a bastion node for connecting to the Kubernetes cluster nodes that includes a single control-plane node. The infrastructure is deployed on AWS. You can log in to AWS using the lab credentials and Open Environment link when needed.

The following instructions connect to the cluster using a web browser terminal. You can also connect using your preferred SSH client rather than the browser terminal by using the PPK (Windows) or PEM (Mac/Linux) key files in the Credentials section of this lab to connect to the bastion host's IP address which appears below the credentials once it is available.

Note: The Open Environment link to the left takes you to AWS, while the Open Development Env. link takes you to the web terminal.

 

Instructions
1. Click Open Development Env. once you see 100% Setup completed:

alt

This connects you to the browser terminal.

 

2. At the login prompt enter the following credentials:

login: Enter ca
Password: Press enter (no password)
You are then presented with the shell prompt:

alt

 

3. Once you see Open Environment 100% Setup completed (It usually takes 1 minute from the time you start the lab):

alt

Copy the Cluster SSH command that appears at the bottom of the lab Credentials panel:

alt

 

4. In the browser terminal, enter the Cluster SSH command at the prompt:

You are now connected to the bastion node in the cluster:

alt

 

5. To ensure the cluster has one available worker node (ROLES set to <none>) before proceeding to use the cluster, enter the following command to watch the cluster come up and press ctrl+c to stop watching: 

Copy code
1
watch kubectl get nodes
alt

It usually takes 2 minutes from the time you start the lab to see a worker node join the cluster. You can proceed to the next step before seeing the worker node, but one worker node should be available before you start working with the cluster.

Note: You may see The connection to the server 10.0.0.100:6443 was refused - did you specify the right host or port? while the control plane's API server prepares to accept requests. It is not a problem and the control-plane node will appear before long.

Understanding Kubernetes Authentication

Introduction
All requests sent to Kubernetes go through the Kubernetes API, which is hosted by the Kubernetes API Server. When you use kubectl to create a Deployment, kubectl sends a request to the API Server to create it. The Deployment object gets stored in the cluster's data store (etcd) once the request is accepted and the cluster controllers begin to create the Deployment's Pods. However, before the request is accepted, the API Server must allow it through its three layers of access control depicted below:

alt

Authentication: Requests sent to the API server are authenticated to prove the identity of the requester, be it a normal user or a service account, and are rejected otherwise.
Authorization: The action specified in the request must be in the list of actions the authenticated user is allowed to perform or it is rejected.
Admission Control: Authorized requests must then pass through all of the admission controllers configured in the cluster (excluding read-only requests) before any action is performed.
In this lab step, you will explore authentication in the lab's Kubernetes cluster. How does kubectl know who is sending the request? How does the cluster recognize the sender? These are the questions that are answered in this lab step.

 

Instructions
1. Use kubectl to get the Pods in the default Namespace:

Copy code
1
kubectl get pods
alt

No Pods are currently running. However, you will investigate how the request was authenticated which is the first layer in the API Server's access control.

 

2. Re-issue the same command but with log level 6 verbosity (--v=6):

Copy code
1
kubectl get pods --v=6
alt

The log Config loaded from file indicates the configuration file used by kubectl. This file is referred to as the kubeconfig file and is in the default location (.kube/config in the user's home directory). The following log summarizes the REST API request sent to the API Server. The kubeconfig file includes:

Information about the cluster, such as the server address
Information about users to authenticate as including certificates
You will see this next.

 

3. Display the contents of the kubeconfig file:

Copy code
1
cat /home/ubuntu/.kube/config
The file is in YAML format. Most of the contents are certificate and key data, but observe that there are several top-level keys including clusters, contexts, current-context, and users.

The users list includes one user (kubernetes-admin) and provides a client certificate (client-certificate-data) and client key (client-key-data) to authenticate requests (certificate and key values omitted in the screenshot): 

alt

Client key and certificate is the default way to authenticate requests in Kubernetes clusters although other authentication modules are supported, including passwords and tokens. A request will pass authenticate if any of the authentication modules successfully authenticates the request.

The following image highlights the contexts and current-context keys:

alt

A context is a triple of a cluster (kubernetes), a user (kubernetes-admin), and a Namespace (if not specified, the default Namespace is used). The context is also given a name for reference (kubernetes-admin@kubernetes). The current-context sets the context that will be used by kubectl by default. To manage the configuration of kubectl, you can use its config command.

 

4. View the config commands provided by kubectl:

Copy code
1
kubectl config --help
alt

These commands can be used to safely write to kubeconfig files using the delete, set, unset, and use commands. 

 

5. Enter the following config command to get a summary of the contexts available in a kubeconfig file:

Copy code
1
kubectl config get-contexts
alt

This view is useful for showing you all of the configured contexts and which is the CURRENT context. If there is no current context set, there will be no * in the first column. In this lab, there is only one context, but you could have several contexts in practice. There are also multiple contexts for different clusters in the Kuberenetes certification exams. If the current context is not what you want, you can use the use-context command to change it.

 

6. Extract the user certificate from the kubeconfig file, and use OpenSSL to show the certificate details:

Copy code
1
2
3
4
5
grep "client-cert" ~/.kube/config | \
  sed 's/\(.*client-certificate-data: \)\(.*\)/\2/' | \
  base64 --decode \
  > cert.pem
openssl x509 -in cert.pem -text -noout
For this lab, the line to focus on is as follows:

alt

The Subject shows the kubernetes-admin common name (CN) and system:masters organization (O). Kubernetes maps the common name to users and the organization (if present) to groups. The cluster can verify the certificate is valid and therefore any request using the certificate is authenticated as the kuberentes-admin user.

 

7. Back up the kubeconfig before removing the user and observing the impact on a request:

Copy code
1
2
3
cp .kube/config .kube/config.orig
sed -i '15,$d' .kube/config
kubectl get pods --v=6
alt

Without a certificate username and password authentication is fallen back to.

 

8. Enter any username and password at the prompts and observe the request is Forbidden:

alt

No users are configured for password authentication in the cluster so any combination of username and password will be forbidden. The same result would happen if a client certificate expired or was invalid for another reason.

 

9. Press ctrl+c to end the credential prompts and replace the kubeconfig with it's original:

Copy code
1
cp .kube/config.orig .kube/config
You have focused on user authentication so far but ServiceAccounts go through the same authentication process. However, ServiceAccount use tokens for authentication which are stored in Secrets.

 

10. View the default ServiceAccount's token Secret:  

Copy code
1
kubectl get serviceaccounts default -o yaml
At the bottom of the output the token, which is stored as a Secret is displayed:

alt

 

11. View the value of the Secret by entering the following:

Copy code
1
kubectl get secrets -o yaml
alt

This is the token that the ServiceAccount uses to authenticate its requests.

 

Summary
In this lab step, you learned how authentication works in Kubernetes, and in particular when using kubectl for users and ServiceAccounts. User requests must pass the same authentication and other access control layers regardless of if they are sent from kubectl, using client libraries or REST API requests (in fact kubectl and client libraries are also sending REST API requests but abstracting the details away from you). You understood the role of kubeconfig files, contexts, and config commands to ensure that kubectl is properly configured to communicate with the target cluster. You also saw how ServiceAccounts use tokens for authentication.

ubuntu@ip-10-0-128-5:~$ history
    1  watch kubectl get nodes
    2  kubectl get pods
    3  kubectl get pods --v=6
    4  cat /home/ubuntu/.kube/config
    5  kubectl config --help
    6  kubectl config get-contexts
    7  grep "client-cert" ~/.kube/config |   sed 's/\(.*client-certificate-data: \)\(.*\)/\2/' |   base64 --decode   > cert.pem
    8  openssl x509 -in cert.pem -text -noout
    9  cp .kube/config .kube/config.orig
   10  sed -i '15,$d' .kube/config
   11  kubectl get pods --v=6
   12  cp .kube/config.orig .kube/config
   13  kubectl get serviceaccounts default -o yaml
   14  kubectl get secrets -o yaml
   15  history
ubuntu@ip-10-0-128-5:~$

Understanding Kubernetes Authorization

Introduction
The second layer in Kubernetes API access control is authorization. At this stage the request is authenticated and the requesting user is known. Authorization needs to determine if the authenticated user is allowed to perform the action requested. The default authorization mechanism in Kubernetes is role-based access control (RBAC). In RBAC, subjects (users, groups, ServiceAccounts) are bound to roles and the roles describe what actions the subject is allowed to perform. There are two kinds of roles in Kubernetes RBAC:

Role: A namespaced resource specifying allowed actions
ClusterRole: A non-namespaced resource specifying allowed actions
Roles include a Namespace where the actions are allowed in contrast to ClusterRoles which don't include a Namespace and can be bound to any and all Namespaces. ClusterRoles can also allow actions on non-namespaced resources, such a Nodes.

There are two resources available for binding roles to subjects:

RoleBinding: Bind a Role or ClusterRole to a subject(s) in a Namespace
ClusterRoleBinding: Bind a ClusterRole to a subject(s) cluster-wide
You will understand the basics of Roles and RoleBindings in this lab step. By the end of this lab step you will know why the kubernetes-admin user is authorized to perform any action in the cluster and learn how to check whether a subject is allowed to perform an action using kubectl.

 

Instructions
1. List the Roles in all Namespaces:

Copy code
1
kubectl get roles --all-namespaces
alt

The Roles all have an associated Namespace in the first column. It is a best practice to follow the principle of least privilege and ensure subjects don't have more access than they need. The roles in the list are related to specific applications/controllers and provide the least access required to perform their responsibilities. 

 

2. View the Role object for the kube-proxy Role in the kube-system Namespace:

Copy code
1
kubectl get -n kube-system role kube-proxy -o yaml
The rules map contains the allowed actions:

alt

Rules follow the same structure as the example above. The example allows read (get) access to ConfigMaps (configmaps) named kube-proxy. The verbs declare which HTTP verbs are allowed for requests. The Kubernetes API is organized into groups and the apiGroups list indicates which API group(s) the rule applies to. The core API group which includes the most commonly used resources, including ConfigMaps, is denoted by an empty string ("").

The rules for ClusterRoles follow the same structure.

 

3. List all of the cluster roles:

Copy code
1
kubectl get clusterroles
alt

Kubernetes creates several cluster roles by default. The one that is germane to this exercise is the cluster-admin role.

 

4. Show the cluster-admin ClusterRole resource YAML:

Copy code
1
kubectl get clusterrole cluster-admin -o yaml
alt

The rules key allows all actions (verbs) on all resources. Access to any non-resource URLs (nonResourceURLs)is also allowed providing full access to the cluster. You can also get the same information from kubectl describe clusterrole cluster-admin. 

 

5. Describe the cluster-admin ClusterRoleBinding to understand how the cluster-admin ClusterRole is bound to users:

Copy code
kubectl get clusterrolebinding cluster-admin -o yaml
alt

The RoleRef map specifies the name of the ClusterRole that is being bound, and the subjects map lists all the subjects (users, groups, or service accounts) that are bound to the ClusterRole. In this case, the ClusterRole is bound to a Group named system:masters. Because identities are managed outside of Kubernetes, you cannot use kubectl to show details of users or groups. However, recall that the client certificate used in the kubeconfig identifies the user as kubernetes-admin and the group as system:masters. Because the kubernetes-admin is in the system:masters group, the cluster-admin ClusterRole allows any request sent.

In general, you can't always assume every request will be allowed as is the case for cluster admins. You can use kubectl to tell you whether a subject is allowed to make a request as you will see next.

 

6. Confirm the kubernetes-admin user is authorized to list nodes:

Copy code
1
kubectl auth can-i list nodes
alt

The can-i command will return a binary response for whether or not a user is authorized to perform a specified action. You can learn more about the actions from the command's help page (kubectl auth can-i --help). As a cluster admin you are authorized to impersonate other users which allows you to see what other users are authorized to do.

 

7. Check if the user Tracy is allowed to list nodes:

Copy code
1
kubectl auth can-i list nodes --as=Tracy
alt

The --as option of kubectl allows user impersonation. There is no role binding for Tracy so the request is not Authorized.

 

Summary
In this lab step, you reviewed Roles/ClusterRoles and RoleBindings/ClusterRoleBindings, and how they allow the kubernetes-admin user to perform any action in the cluster.

